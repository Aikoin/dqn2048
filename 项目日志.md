## 06/06 周六

完成2048环境的搭建

![批注 2020-06-06 211802](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200610153939.png)

命令行也不是不能玩哈😂😂

## 06/07 周日

算法策略超参数![IMG_20200609_150532](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200610153945.jpg)

代码逻辑梳理![IMG_20200609_150513](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200610154000.jpg)

## 06/08 周一

下午第一版DQN开始训练

出现了由非法动作引起的死循环bug😂😂

## 06/09 周二

训练了近 6w+ eposide后效果如图：

![image-20200609145344240](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200610154015.png)

学到了点东西，但是效果还是很垃圾；自己电脑跑真的太慢了，得找个云平台

## 06/10 周三

- 改了训练的策略，以前是每隔400次迭代训练一次，改成只要达到经验池就每个迭代都训练一次

- 极算云训练了一晚上训练到4w+感觉和我跑的结果差不多啊，算了下score的均值只有1800+，这也太差了

- 继续增加训练过程的中必要参数输出 , 包括game的matrix，epsilon输出等

- 跑了一下其他人的版本，1w+就已经可以稳定在五六k的score了，是我太菜了

  ![image-20200610102525862](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200610105129.png)

- 我找到问题所在了！！它没有随机选择而是按顺序批次喂入网络，它每次得到的都是Q向量，对于非法动作的label直接置0！！这也将我的两个点解决了，还是之前没有好好看代码就上手了！！

- 关键参数：8k eposide到0.1，1w多步learningrate到0.0002

## TODO

- [x] 添加输出训练过程中必要信息，max_num和max_score

- [x] 支持中断训练后恢复，实时保存Class所有信息（用jupyter很方便地完成，不用改代码）

- [ ] 尝试修改reward机制，尝试其他优化器，尝试lr的衰减

- [ ] 尝试更改模型，用2015版dqn以及后续优化版本

- [x] 转到极算云训练

- [x] ！！！发现一个非常大的提升的点，刚开始的策略训练轮次太低了

  > 然后只要经验回放池里的数据量大于我们用于需要采样的样本量BATCH_SIZE，就去训练Q网络。

- [ ] 非法动作的处理

  - 现在的处理是：e-greedy不会选择非法动作，但是在更新的时候用maxQ(s,a)时可能会用到非法动作的状态价值，这个会不会导致很大的偏差，因为非法动作的价值函数是不会直接通过反向传播更新的
  - 一个预想的处理方式是：在动作价值函数的时候也避免非法动作，这个逻辑对代码改动比较大
  - 另一个预想的处理方式是：允许执行非法动作，状态不变；和常规动作相比不施加奖励或者惩罚；但是依然可能有无限循环的风险！！！

- [ ] reward的处理

  - 现在的处理方式是：增加的分数取log除以15，没有合并的话是不会有奖励的，这个设定从理论上来说没有问题，会不会有更好的方案呢；
  - 设想方案1：和非法动作的处理结合起来考虑，执行非法动作reward为负；执行常规动作奖励为0；成功合并后施加奖励。
  - 设想方案2：额外奖励合并更大的方块，额外奖励方块数量少（整洁）
  - 更大的想法3：通过奖励来学习我自己玩2048的策略，S型方法理想情况下（感觉违背了强化学习的初衷）